{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering:\n",
    "  title:\n",
    "    offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch1.3)=\n",
    "# The Rules of Chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all outcomes are equally likely, not all systems are symmetric or close to symmetric, and, while we can build outcome spaces without equally likely outcomes from outcome spaces with equally likely outcomes, it is rarely worthwhile to expand the natural description of outcomes into a detailed description in pursuit of equal likelihood. In many cases, the virtue of a probability model is its ability to match observed frequencies, and to derive consequences of those frequencies, without requiring a detailed explanation for the mechanism that produced them.\n",
    "\n",
    "What we want is a more general theory that allows any outcome space, and any assignment of chance to events, such that the assignment of chance to events could be equated to frequencies of outcomes in a string of repeated trials, or to proportions in a more detailed symmetric model. Thankfully, many models can satisfy these requirements as long as they satisfy three simple rules. These rules are the heart of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Probability Axioms\n",
    "\n",
    "A **probability model** is a procedure that returns an answer for any question of the kind, *\"What is the probability of the event $E$?\"* The procedure should provide an answer for any event $E$ in an outcome space. Since events are subsets of $\\Omega$, the probability model must specify a rule that accepts sets as inputs and returns chances. Since chances are meant to model proportions, or frequencies, they are real valued numbers between zero and one. The function that accepts any subset of $\\Omega$, and returns a real number representing its chance is the **probability measure**. A **measure** is any function that accepts all subsets of an outcome space $\\Omega$ and returns a real number. \n",
    "\n",
    "We've already fixed notation for the probability measure, since we defined $\\text{Pr}(E)$ to be the probability of the event $E$. Therefore, $\\text{Pr}$ is the probability measure. The probability model is the combination of the outcome space $\\Omega$, the collection of all its subsets that we might ask about as events, and the probability measure $\\text{Pr}$ that returns the chance of any event we ask about. \n",
    "\n",
    "A probability model is valid if the measure satisfies three rules. These are Kolmogorov's probability axioms. The first two are not surprising, since chances are meant to model proportions or frequencies. The last is the most substantive. We observed that it was true for proportions (see [Section 1.2](#ch1.2)).\n",
    "\n",
    ":::{note} The Probability Axioms\n",
    "1. **Nonnegativity:** $\\text{Pr}(E) \\geq 0$.\n",
    "    - The probability of any event is greater than or equal to zero. \n",
    "    - In other words, chances cannot be negative. \n",
    "\n",
    "1. **Normalization:** $\\text{Pr}(\\Omega) = 1$.\n",
    "    - One of the outcomes in $\\Omega$ must occur. \n",
    "\n",
    "1. **Additivity:** If $A$ and $B$ are disjoint events then $\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B)$\n",
    "    - The probability that either $A$ or $B$ happen, equals the probability $A$ happens plus the probability $B$ happens, if $A$ and $B$ cannot occur simultaneously.\n",
    "    - This is a sensible conclusion for proportions. If $A$ and $B$ are disjoint, then the number of outcomes in either $A$ or $B$ equals the number of outcomes in $A$ plus the number of outcomes in $B$. If $A$ and $B$ are not disjoint, i.e. are not mutually exclusive, then they could co-occur, so they share some outcomes. In that case, the number of elements in either $A$ or $B$ is *less* than the number of elements in $A$ plus the number of elements in $B$.\n",
    "    - ‚ö†Ô∏è So, only apply this addition rule **when the events are mutually exclusive.** We will explore the case when the events overlap at the end of this section.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Distributions\n",
    "\n",
    "While the axioms provide some basic rules the measure $\\text{Pr}(\\cdot)$ must satisfy, they give no guidance on the actual choice of measure. The axioms alone cannot determine the chance of an event. To determine a chance we need a model.\n",
    "\n",
    "We saw this same conceptual division in the previous chapter. \n",
    "\n",
    "- Assuming equally likely outcomes fixed a model since it assigned chances to individual outcomes. It did not give any direction on how those chances combine, or how to compute the probability of an event containing multiple outcomes. \n",
    "- Asserting a relationship between probability and long run frequency did not provide a rule for computing chances. It did give explicit direction on how chances combine, namely, chances for disjoint events add.\n",
    "\n",
    "We put these rules together to build up probability as proportion. By assigning a chance to each outcome, then by adding chances together to find the probability of events, we saw that, if outcomes are equally likely, then the probability of an event is the fraction of all possible outcomes that satisfy the event definition. We can repeat the same process by applying the axioms to any rule that assigns a well-defined chance to every outcome.\n",
    "\n",
    "A **categorical distribution** is a rule that assigns a chance to every distinct outcome in $\\Omega$ when $\\Omega$ is finite. Then, by the additivity axiom:\n",
    "\n",
    "$$\\text{Pr}(E) = \\sum_{\\omega \\in E} \\text{Pr}(\\omega) $$\n",
    "\n",
    "You should read the equation above as: *the probability of any event ($E$) is the sum, over all the ways the event can happen ($\\omega \\in E$), of the probability of each way the event can happen ($\\text{Pr}(\\omega)$).\"* In other words, the chance of an event is the total chance of all the outcomes contained in the event. \n",
    "\n",
    "Categorical distributions are the most directly defined probability models; simply assign a chance to every outcome. The equally likely model we studied before is a **uniform** categorical distribution. It is uniform since it assigns the same chance to every outcome.\n",
    "\n",
    "Categorical distributions get their name from their application. They are often used to categorize and classify data. \n",
    "\n",
    ":::{tip} Example\n",
    ":class: dropdown\n",
    "Suppose that you were designing a self-driving car. You might want a computer vision system that could distinguish pedestrians, scooters, cyclists, vehicles, and so on. Usually these systems rely on a machine learning model that returns a categorical distribution. It receives a string of images (a video), segments the images into objects, and then, for each object, assigns a chance to the categories pedestrian, scooter, cyclist, vehicle, etc. For a given object it might return a categorical distribution:\n",
    "\n",
    "Event | Ped. üëü | Scoot. üõµ  | Cycl. üö≤  | Veh. üöó |\n",
    ":----:|:---------|:---------|:---------|:---------|\n",
    "Probability | 0.1 | 0.4 | 0.5 | 0 | \n",
    "\n",
    "We often represent categorical distributions with bar charts whose heights equal the chance of each outcome. This is sometimes called a **probability histogram**.\n",
    "\n",
    "![Example Categorical Distribution.](categorical_distribution.svg \"Example Categorical Distribution.\")\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\text{Pr}(\\text{scooter or cyclist}) = 0.4 + 0.5 = 0.9$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\text{Pr}(\\text{not a vehicle}) = \\text{Pr}(\\text{pedestrian or scooter or cyclist}) = 0.1 + 0.4 + 0.5 = 1.$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Axioms\n",
    "\n",
    "### The Complement Rule\n",
    "\n",
    "An axiom is a mathematical statement that is asserted as a basic premise for a theory. Many rich mathematical areas are built by first posing a short list of self-evident (or, at least, plausible) axioms. Once accepted, the axioms are used to prove the rest of the theory. Let's practice this way of thinking using the probability axioms.\n",
    "\n",
    "In [Section 1.2](#ch1.2) we observed that, when we equated probability to proportion, the rule:\n",
    "\n",
    "$$\\text{Pr}(A^c) = 1 - \\text{Pr}(A) $$\n",
    "\n",
    "That is, the probability that an event does not occur equals one minus the probability that it does occur. \n",
    "\n",
    "This rule might appear self-evident. For instance, it may feel instinctive that, if the probability that it rains tomorrow is 1/3, then the probability is doesn't rain should be 2/3. Let's show that we don't need this rule as an additional fourth axiom. Instead, it is implied by the first three.\n",
    "\n",
    ":::{tip} Proof:\n",
    ":class: dropdown\n",
    "\n",
    "1. The sets $A$ and $A^c$ are disjoint for any event $A$. No outcome can simultaneously be in $A$ and to not be in $A$.\n",
    "\n",
    "1. The sets $A$ and $A^c$ partition $\\Omega$ since $A \\cup A^c = \\Omega$. \n",
    "    - If you're not convinced, draw a box representing $\\Omega$, then a circle inside it labeled $A$. First, shade the region inside the circle. This is all outcomes in $A$. Then, shade the region outside the circle. This is all outcomes not in $A$. When you've finished shading $A^c$, you will have shaded the whole box. \n",
    "\n",
    "1. So, by additivity, $\\text{Pr}(A) + \\text{Pr}(A^c) = \\text{Pr}(\\Omega)$.\n",
    "\n",
    "1. Then, by normalization, $\\text{Pr}(A) + \\text{Pr}(A^c) = \\text{Pr}(\\Omega) = 1$.\n",
    "    - Rearranging:\n",
    "\n",
    "    $$\\text{Pr}(A^c) = 1 - \\text{Pr}(A) $$\n",
    "\n",
    ":::\n",
    "\n",
    "Let's add this rule to our list:\n",
    "\n",
    ":::{note} Rules of Chance\n",
    "4. **Complements:** For any event $A$:\n",
    "\n",
    " $$\\text{Pr}(A^c) = 1 - \\text{Pr}(A).$$\n",
    ":::\n",
    "\n",
    "\n",
    "### Bounding The Chance of a Union\n",
    "\n",
    "Our union rule looks different than the complement rule since it does not hold for all pairs of events. What happens if $A$ and $B$ are not disjoint?\n",
    "\n",
    "An example suffices. What is the chance that a single roll of a fair die is even *or* is less than 4. The event that the roll is even contains the outcomes $\\{2,4,6\\}$. The event that it is less than 4 contains the outcomes $\\{1,2,3\\}$. So, the event that the roll is even or less than 4 contains the outcomes $\\{1,2,3,4,6\\}$. There are five of these outcomes so the desired probability is $5/6$.\n",
    "\n",
    "What would have happened if we tried to apply our rule?\n",
    "\n",
    "$$\\text{Pr}(\\text{even}) + \\text{Pr}(\\text{less than 4})  = \\frac{3}{6} + \\frac{3}{6} = 1 \\neq \\frac{5}{6} $$\n",
    "\n",
    "In this case the sum of the individual probabilities is too large. It equals one, which is clearly wrong, since it is possible to roll a value that is neither even nor less than 4. The only such value is a 5, so $\\text{Pr}(\\text{even or less than 4}) = 1 - \\text{Pr}(\\text{odd and greater than or equal to 4}) = 1 - (1/6) = 5/6.$\n",
    "\n",
    "The sum of the individual probabilities is too large since it counts an outcome twice. The outcome where the die lands on 2 appears in both events since 2 is both even and less than 4. In fact, $\\{2\\} = \\{\\text{even and < 4}\\} = \\{2,4,6\\} \\cap \\{1,2,3\\}$. The sum of the probability that the die roll is even and the probability that the roll is less than four counts the outcomes that are even and less than four twice, once for each set. Therefore, it overcounts the intersection of the events:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Pr}(A) + \\text{Pr}(B)  & = \\text{Pr}(A \\text{ but not } B) + 2 \\times \\text{Pr}(A \\text{ and } B) + \\text{Pr}(B \\text{ but not } A) \\\\\n",
    "& = (\\text{Pr}(A \\text{ but not } B) + \\text{Pr}(A \\text{ and } B) + \\text{Pr}(B \\text{ but not } A)) + \\text{Pr}(A \\text{ and } B) = \\text{Pr}(A \\text{ or } B) + \\text{Pr}(A \\text{ and } B)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is the general rule for unions:\n",
    "\n",
    "$$\\text{Pr}(A \\text{ or } B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\text{ and } B) $$\n",
    "\n",
    "Since all probabilities are nonnegative:\n",
    "\n",
    "$$\\text{Pr}(A \\text{ or } B) \\leq \\text{Pr}(A) + \\text{Pr}(B)$$\n",
    "\n",
    "with equality if and only if the intersection has probability zero, $\\text{Pr}(A \\text{ and } B) = 0$. In other words, if and only if the events are mutually exclusive.\n",
    "\n",
    "Let's add this last rule to our list:\n",
    "\n",
    ":::{note} Rules of Chance\n",
    "5. **Union Upper Bound:** For any pair of events $A$ and $B$: \n",
    "\n",
    "$$\\text{Pr}(A \\text{ or } B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\text{ and } B)  \\leq \\text{Pr}(A) + \\text{Pr}(B).$$\n",
    ":::\n",
    "\n",
    "You'll practice with this rule in discussion section and on your HW.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "To summarize our rules, let's append a column to the logic and sets table from [Section 1.1](#ch1.1):\n",
    "\n",
    ":::{tip} Logic, Sets, and Algebra\n",
    "\n",
    "Logical | Set Operation                               | Notation  | Chance Rule |\n",
    ":----:|:-------------------------------------------------|:-------------|:-------------\n",
    "not   | complement                            |$^c$ | $ 1 - ...$|\n",
    "or   | union           |$\\cup$ | $+$ if disjoint |\n",
    "if   | restrict $\\Omega$            |  $\\mid$ | ? |\n",
    "and   | intersect | $\\cap$ | ? |     \n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
