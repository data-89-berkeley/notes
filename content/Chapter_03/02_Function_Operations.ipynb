{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering:\n",
    "  title:\n",
    "    offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch3.2)=\n",
    "# Function Operations\n",
    "\n",
    ":::{caution} Under Construction\n",
    "We have not integrated the demos into the text yet. To start interacting with them, go to this [Notebook](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FDATA-89&branch=main&urlpath=tree%2FDATA-89%2Fbasic_functions_week_3.ipynb).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function operation is a procedure we can apply to transform or combine functions. Function operations are the essential tools that make mathematical modeling expressive. They are also the key to breaking complicated functions down into bite-sized pieces. The better you get at recognizing functions, and the richer your album of mental images, the more efficiently you will be able to break down formula into their pieces, visualize each piece, then visualize their combinations. \n",
    "\n",
    ":::{tip} Strategy\n",
    "Try to expand your function as a transformation, or combination, of simpler pieces. Since visualizing transforms and combinations is hard, use as few pieces as you can.\n",
    ":::\n",
    "\n",
    "## Linear Transformations...\n",
    "1. **to the Input:** These transforms are used to generalize almost every distribution family. They need to be instinctive. \n",
    "    - **Horizontal Translation:** Replace $f(x)$ with $f(x - s)$ to **translate** the function horizontally by a shift $s$. For example, $f(x - 3)$ looks like $f(x)$ shifted horizontally to the right by 3 units. \n",
    "\n",
    "    ![Horizontal Translation](normal_shift_example.svg \"Horizontal Translation\")\n",
    "\n",
    "    - **Dilation:** Replace $f(x)$ with $f(x/a)$ for some $a > 0$ to **dilate** the function. You can think of $a$ as controlling a zoom factor on the horizontal axis.\n",
    "        - Using $a$ less than 1 compresses the function by making it narrower. \n",
    "        - Using $a$ greater than 1 expands the function by making it wider. For example, setting $a = 3$ makes the function three times wider.\n",
    "\n",
    "        ![Horizontal Dilation](normal_dilation_example.svg \"Horizontal Dilation\")\n",
    "\n",
    "        - If $a < 0$ then the result also reflects $f$ about $x = 0$. \n",
    "\n",
    "    - **Generic:** Replace $f(x)$ with $f((x - s)/a)$.\n",
    "\n",
    "1. **to the Output:** We can apply the same operations to the outputs of functions. \n",
    "    - **Vertical Translation:** Replace $f(x)$ with $f(x) + h$ to **translate** the function vertically by a height $h$. For example, $f(x) + 2$ looks like $f(x)$ shifted vertically by 2 units.\n",
    "\n",
    "    ![Vertical Translation](vertical_translation_example.svg \"Vertical Translation\")\n",
    "\n",
    "    - **Vertical Scaling:** Replace $f(x)$ with $c f(x)$ to **scale** the function. You can think of $c$ as controlling a zoom factor on the vertical axis. \n",
    "        - Using $c < 1$ shrinks the function by making it shorter.  For example, replacing $f(x)$ with $\\frac{1}{3} f(x)$ compresses $f$ vertically by a factor of 3.\n",
    "\n",
    "        ![Vertical Scaling](vertical_scaling_example.svg \"Vertical Scaling\")\n",
    "\n",
    "        - Using $c > 1$ expands the function by making it taller.\n",
    "        - If $c < 0$ then the function reflects about the horizontal axis.\n",
    "\n",
    "    - **Generic:** Replace $f(x)$ with $c f(x) + h$.\n",
    "\n",
    ":::{important} Maintaining Normalization\n",
    "\n",
    "All distribution functions must be normalized. For instance, in [Section 2.4](#ch2.4) we saw that, for any PDF:\n",
    "\n",
    "$$\\int_{x = -\\infty}^{\\infty} \\text{PDF}(x) dx = 1. $$\n",
    "\n",
    "It is standard practice to define a generic family of models by setting $\\text{PDF}(x) \\propto g((x - s)/a)$ for some nonnegative function $g$, shift $a$, and horizontal dilation $a$. These parameters are often called a **location** and a **scale** parameter. The location parameter controls the horizontal position of the distribution. The scale parameter controls its breadth. \n",
    "\n",
    "The $\\propto$ notation hides the normalization constant. This is useful, since the essence of a distribution is its shape, which is determined by the functional form $g$. However, when $g$ depends on some free parameters, then we should always remember that:\n",
    "\n",
    "$$\\text{PDF}(x) = c(s,a) g((x - s)/a) $$\n",
    "\n",
    "for some constant $c(s,a)$ that also depends on the parameters. \n",
    "\n",
    "The location parameter has no effect on the normalizing constant since it just shifts the distribution. The scale parameter does. It can make the distribution wider or narrower. Just like a rectangle, if we make a distribution twice as wide, we double its area. So, to keep the distribution normalized, we must also always make it twice as short. \n",
    "\n",
    "Generically:\n",
    "\n",
    "$$\\text{PDF}(x) = \\frac{C}{|a|} g \\left(\\frac{(x - s)}{a} \\right) $$\n",
    "\n",
    "where $C$ is just a number determined by $g$ ($C = 1/(\\int_{x = -\\infty}^{\\infty} g(x) dx)$). That way, if we make the distribution wider by adjusting the scale parameter we also make it shorter.\n",
    "\n",
    ":::\n",
    "\n",
    "Run the code cell below to visualize linear transformations of the inputs and outputs of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Combinations:\n",
    "\n",
    ":::{tip} Decomposing Functions\n",
    "Every function studied in this class can be broken into simpler parts that are combined either by an algebraic operation or via a composition. Look for ways to break functions into recognizable parts. \n",
    ":::\n",
    "\n",
    "1. **Algebraic Combination:** \n",
    "    - **Function Addition and Multiplication:** As they sound, $f(x) + g(x)$ or $f(x) \\times g(x)$. \n",
    "        - Visualize the function sums like a stacked plot where the two functions sit on top of one another. \n",
    "        - Visualizing function products takes practice, and is often best left to the tools from [Sections 3.1](#ch3.1) and [Section 3.3](#ch3.3). When given a product, always check the roots and sign of each term separately. Unfortunately, many distributions are expressed as products of functions. \n",
    "    - **Linear Combination:** This is a special version of function addition. It looks like $a f(x) + b g(x)$ for some coefficient $a$ and $b$ that scale each term in the combination.  \n",
    "        - You can visualize a linear combination either by drawing its two component functions, $a f(x)$ and $b g(x)$ separately, then adding them together to produce the combo. The green and blue bumps are the component functions. The red curve is their linear combination. Varying $a$ or $b$ makes the associated bumps taller or shorter.\n",
    "\n",
    "        ![Linear Combination](Linear_Combination.png \"Linear Combination\")\n",
    "\n",
    "        - Alternately, you can use a stacked plot convention where you first draw $a f(x)$, then you draw $a f(x) + b g(x)$ where the difference between your first curve and your second curve is $b g(x)$. Here's the same combination, using a stacked convention. In this example we drew the blue bump first, then added the green bump on top of it.\n",
    "\n",
    "        ![Linear Combination Stacked](Linear_Combination_Stacked.png \"Linear Combination Stacked\")\n",
    "\n",
    "        - Important examples in probability are mixture distributions.\n",
    "\n",
    "        :::{tip} Mixture Distributions\n",
    "        :class: dropdown\n",
    "\n",
    "        To construct a mixture, sample in stages. \n",
    "        \n",
    "        For example, suppose that we have two large populations. We first pick a population to sample from at random, then, from that population, draw a sample of $n$ individuals. Then, we count the number of the sampled individuals who have a characteristic of interest and call our count $X$. Suppose that, in the first population, 2 in 5 individuals have the characteristic, and in the second, 3 of 4 do. \n",
    "        \n",
    "        This process can be modelled as follows. First, draw a Bernoulli random variable $I \\sim \\text{Bernoulli}(p)$ where $p$ is the chance we select the second sample population. Then, if $I = 0$, draw $X \\sim \\text{Binomial}(n,2/5)$. If $I = 1$, draw $X \\sim \\text{Binomial}(n,3/4)$. Note, these shouldn't be exactly Binomial since we usually sample without replacement, but, if the population is much larger than $n$, its not a bad estimate.\n",
    "        \n",
    "\n",
    "        Then, what is the PMF for $X$? Well, we can find the chance that $X = x$ by partitioning, then using the multiplication rule. Alternately, draw an outcome tree. In either case:\n",
    "\n",
    "        $$\\begin{aligned}\n",
    "        \\text{PMF}(x) & = \\text{Pr}(X = x) = (1 - p) \\left(\\begin{array}{c} n \\\\ x \\end{array} \\right) \\left(\\frac{1}{5} \\right)^x \\left(\\frac{4}{5} \\right)^{n - x}+  p \\left(\\begin{array}{c} n \\\\ x \\end{array} \\right) \\left(\\frac{2}{5} \\right)^x \\left(\\frac{3}{4} \\right)^{n - x}  \\\\ & = (1 - p) \\text{PMF}_{X|I = 0}(x) + p \\text{PMF}_{X|I = 1}(x)\n",
    "        \\end{aligned}$$\n",
    "\n",
    "        Here $\\text{PMF}_{X|I = 0}(x)$ is the PMF when we draw from the first population, and $\\text{PMF}_{X|I = 1}(x)$ is the PMF when we draw from the second. The resulting PMF is a *mixture* of the two PMF's since it is a linear combination of the two.\n",
    "\n",
    "        Here's an example with $p = 0.3$ and with $n = 20$. The colors represent the component distributions.\n",
    "\n",
    "        ![Binomial Mixture](binomial_mixture.png \"Binomial Mixture\")\n",
    "\n",
    "        :::\n",
    "\n",
    "\n",
    "Run the code cell below to visualize function addition and multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Function Composition:** The **composition** of $h$ and $g$ is $h \\circ g(x) = h(g(x))$. Many distributions are expressed as compositions. \n",
    "    - To visualize an arbitrary function composition, proceed as follows: \n",
    "\n",
    "        :::{tip} Drawing composites\n",
    "        1. Draw the inner function, $g(x)$, and the outer function $h(x)$. Clearly distinguish them with different colors or markers so you don't mix them up. \n",
    "        1. Add to your plot the line $y = x$. This line is useful since we can use it to exchange inputs and outputs.\n",
    "        1. Work an input at a time. Pick some $x$. Add a point at $(x,0)$ on the x-axis. Trace a lightly dashed line vertically upwards so you can remember where you started.\n",
    "            - Next, add a point at $(x,g(x))$ where your dashed vertical meets $g(x)$. We've now produced the output of the inner function. \n",
    "            - To pass the output of the inner function into the input of the outer function, trace horizontally across from $(x,g(x))$ to $(g(x),g(x))$. This is the intercept between the horizontal line passing through $(x,g(x))$ and the $y = x$ line. Then, trace vertically from $(g(x),g(x))$ to $(g(x),h(g(x)))$. This is the intercept of a vertical line leaving $(g(x),g(x))$ and intersecting the outer function $h$. \n",
    "            -  We've now recovered $h(g(x))$. To plot it at the correct input, trace horizontally until you intercept the lightly dashed line leaving the original $x$. That is, from $(g(x),h(g(x)))$ to $(x,h(g(x)))$.\n",
    "        :::\n",
    "\n",
    "    - This process is a bit involved at first, but it's a nice visual procedure. Once you get the hang of it, you can use it to very quickly evaluate compositions of arbitrary $h$ and $g$. Just repeat the process for a bunch of different $x$ values. It is good practice to try this by hand at least once. \n",
    "    \n",
    "Run the code cell below to visualize the composition of two functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed orange lines represent the procedure provided above. Try building up an example composition. A good place to start is $f(x) = e^{-\\frac{1}{2} x^2 + 1}$ where the inner function $g$ is a negated quadratic and the outer function is an exponential. You'll practice with this example in discussion.\n",
    "\n",
    "Here's a different example with $f(x) = h(g(x))$ with $g(x) = 0.2 \\times(1  + x^2)$ and $h(x) = 1/x$.\n",
    "\n",
    "![Composite](Composite.png \"Composite\")\n",
    "\n",
    ":::{tip} Building Distributions as Composites\n",
    ":class: dropdown\n",
    "\n",
    "These are both examples where the inner function is convex or concave, and the outer function is both monotonic and nonnegative. This recipe *inner concave*, *outer monotonically increasing and nonnegative* is a good procedure for building density functions. The outer function ensures that the composition returns a nonnegative number. It is usually selected so that it converges to zero in a limit that can be achieved by the inner function. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverses:\n",
    "\n",
    "If $f$ is monotonic, then it is invertible. Its **inverse**, $f^{-1}$ is the function that accepts outputs of $f$ and returns the matching input.\n",
    "- In other words, given $f(x) = y$, $f^{-1}(y) = x$. \n",
    "- It can help to think, *whatever $f$ does, $f^{-1}$ undoes*. \n",
    "\n",
    "- Inverse are constructed by reflecting $f$ about the $x = y$ line (exchange inputs and outputs).\n",
    "    - To reflect, do the following:\n",
    "\n",
    "        :::{tip} Drawing Inverses\n",
    "        1. Draw $f(x)$.\n",
    "        1. Draw the line $y = x$ which exchanges inputs and outputs.\n",
    "        1. Sketch the reflection of $f$ across $y = x$. \n",
    "        1. If you struggle to sketch the reflection, work one input at a time:\n",
    "            - Select some $x$. Add a point at $(x,f(x)).$\n",
    "            - Trace horizontally to $(f(x),f(x))$. This is the intercept of the horizontal line through $(x,f(x))$ with the $y = x$ line. \n",
    "            - Trace vertically from $(x,f(x))$ to $(x,x)$. \n",
    "            - You now have two sides, and three corners, of a square. Complete the square by adding in the missing corner at $(f(x),x)$. You have now swapped the inputs and outputs of $f$. This new point is the reflection. \n",
    "        1. Repeat this process for many inputs. The resulting curve is the inverse function since it accepts outputs of $f$, and returns the matching inputs. \n",
    "        :::\n",
    "\n",
    "    - The image below shows an example. The blue function if $f$, the dashed grey line is the $y = x$ line that matches inputs and outputs, and the red curve is the inverse produced by reflecting across $y = x$. The orange filled square is the square used to build the reflection.\n",
    "\n",
    "![Inverse Example](Inverse_function.png \"Example Inverse\")\n",
    "\n",
    "The most important examples in probability are the *exponential* and *logarithm* functions. Remember $e^{\\log(x)} = x$ and $\\log(e^{x}) = x$.\n",
    "\n",
    ":::{tip} Exercise ðŸ› ï¸\n",
    "\n",
    "Try drawing the functions $e^x$ and $\\log(x)$.\n",
    "\n",
    ":::\n",
    "\n",
    "Run the code cell below to visualize function inverses. Start with a linear function, and see how the inverse varies as we vary the initial function. The square you see represents the graphical construction outlined above. It is good practice to try this by hand at least once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've run the code below, go back to the composition demo provided above, and pick inner and outer functions that are related by an inverse. For example, $e^x$ and $\\log(x)$. Then, the graphical construction used to create the composite will trace the boundary of the reflecting square, always returning the $(x,x)$ corner. In other words, $f^{-1}(f(x)) = x$. \n",
    "\n",
    "\n",
    ":::{tip} Applications of Inverses in Probability\n",
    "\n",
    "Why care about inverses?\n",
    "\n",
    "Inverse functions are extremely useful in applied problems. Here are two:\n",
    "\n",
    "**Finding Thresholds for Statistical Tests:** It is common practice to run a statistical test by collecting some data, using it to compute a test statistic (e.g. the sample mean), then to compare the observed value of the test statistic to a threshold. Since data is almost always random, the observed test statistic is random, so we'll denote it $T$. We'll denote the threshold $t_*$. Often we pick the test statistic so that its value measures how much the observed data disagrees with what we would expect, or is typical, under some hypothesis. Usually, the larger the test statistic, the more evidence we have that the hypothsis is false. Formally, we pose a chance model for $T$ that should hold under the hypothesis. Then, we select the threshold so that, if the hypothesis were true, then $\\text{Pr}(T \\leq t_*) = 1 - \\alpha$ for some desired $\\alpha$ close to zero. That way, if we observe $T > t_*$, then the observed data would have been suspiciously atypical had our hypothesis been true. This is usually considered statistical evidence against the hypothesis. The chance, $\\text{Pr}(T > t_*)$ is the (inf)famous \"p\"-value. \n",
    "\n",
    "The level $\\alpha$ controls how conservative, and how sensitive, our test is. You can think of it as the chance that the test falsely rejects if the hypothesis is ture.  It is the level of evidence we demand in order to reject the hypothesis. \n",
    "\n",
    "We usually start by fixing an $\\alpha$ (e.g. $\\alpha = 0.05$ or $\\alpha = 0.01$), then solve for the associated threshold $t_*$. Notice that, $\\text{Pr}(T \\leq t_*) = \\text{CDF}(t_*)$. Then, our original equation was:\n",
    "\n",
    "$$\\text{CDF}(t_*) = 1 - \\alpha $$\n",
    "\n",
    "so, to find the desired threshold, we should use:\n",
    "\n",
    "$$t_* = \\text{CDF}^{-1}(\\alpha). $$\n",
    "\n",
    "**Sampling:** Suppose that you wanted to draw a random variable $X$ with a specific CDF, $F_x$. How would you do it?\n",
    "\n",
    "Here's an algorithm:\n",
    "\n",
    "1. First sample a uniform random number $U \\sim \\text{Uniform}([0,1])$. Most pseudorandom number generators due this by exploiting some of the properties of continuous random variables introduced in [Section 2.3](#ch2.3). Namely, if we draw some continuous random variable $Y$, then, no matter its PDF, if we drop the first $d$ digits of $Y$ and multiply by $10^{(d - 1)}$ then we'll get a random variable between 0 and 1 that is essentially uniform if $d$ is big enough. Continuous random variables look uniform on sufficiently small intervals. In practice, many computers look up the time when you trigger a computation, drop most of the leading digits, then use the remainder to make a uniform number.\n",
    "\n",
    "1. Convert the uniform random variable into the desired random variable $X$ via $X = \\text{F_x}^{-1}(U)$.\n",
    "\n",
    "Why does this work?\n",
    "\n",
    "Well, let's find the CDF of $X$:\n",
    "\n",
    "$$\\text{CDF}(x) = \\text{Pr}(X \\leq x) = \\text{Pr}(F_X^{-1}(U) \\leq x) = \\text{Pr}(U \\leq F_X(x)) $$\n",
    "\n",
    "Then, since $U$ is uniform we can use probability by proportion:\n",
    "\n",
    "$$\\text{CDF}(x) = \\text{Pr}(U \\leq F_X(x)) = \\frac{|F_X(x) - 0|}{|1 - 0|} = F_X(x).$$\n",
    "\n",
    "Notice, this will work for both discrete and continuous variables since it is based on the CDF, which is defined in the same way for both.\n",
    "\n",
    "\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
