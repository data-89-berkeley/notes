{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering:\n",
    "  title:\n",
    "    offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2.2)=\n",
    "# Discrete Models\n",
    "\n",
    "This section will explore three famous discrete models. All of the models study random counts. These are random variables that take on integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli \n",
    "\n",
    "The simplest example of a random variable is a random variable that can only take on one of two values. When those values are 0 and 1, we call the random variable a **Bernoulli** random variable. \n",
    "\n",
    "Since $X$ can either equal 1, or equal 0, we can specify the PMF of $X$ using only the chance that $X = 1$, $\\text{Pr}(X = 1) = p$. The value $p$ is the **success probability**. By the complement rule, the chance $X = 0$ is, $\\text{Pr}(X = 0) = 1 - p$.  The value $q = 1 - p$ is the **failure probability**.\n",
    "\n",
    ":::{note} Bernoulli Random Variables\n",
    "A random variable $X$ is drawn from a **Bernoulli distribution** with **success probability** $p$ if:\n",
    "\n",
    "- $X \\in \\{0, 1\\}$\n",
    "\n",
    "- $\\text{Pr}(X = 1) = p$\n",
    ":::\n",
    "\n",
    "The sentence: *the random variable $X$ drawn from a Bernoulli distribution with success probability $p$* is usually expressed:\n",
    "\n",
    "$$X \\sim \\text{Bern}(p) $$\n",
    "\n",
    "where $\\sim$ is the symbol for \"drawn from\". \n",
    "\n",
    "Notice that:\n",
    "\n",
    "1. Every Bernoulli random variable is drawn from a categorical distribution on two categories, labelled \"0\" and \"1\". Since the labels are arbitrary, Bernoulli random variables are often used to study settings where we are interested in a binary event. For example:\n",
    "\n",
    "    - Flip a coin. Label heads 1 and tails 0.\n",
    "\n",
    "    - Run an experiment. Label a succesful outcome 1 and a failure 0.\n",
    "\n",
    "    - Check a statement. Label a valid check 1 and an erroneous check 0. \n",
    "\n",
    "2. Bernoulli random variables are often constructed from *indicators*. An **indicator function** for an event $E$, is the function that returns 1 if $E$ happens, and 0 otherwise. We'll see that many count variables can be expanded as combinations of indicators, where each indicator indicates whether or not some event occurs. \n",
    "\n",
    "3. Bernoulli random variables are specified by the choice of the parameter $p$. If we plot the PMF, then $p$ is the height of the bar assigned to the outcome $X = 1$.\n",
    "   \n",
    "    - The parameter $p$ is often called a success probability since we often use Bernoulli random variables as indicators for events we consider successes. For instance, if I am interested in whether a string of coin tosses contains at least 5 heads, I could define an event $E$, which contains all strings with at least five heads and set $X$ to an indicator for the event. Then, when the event occurs (a success), my indicator equals 1. So, the chance of success is $\\text{Pr}(X = 1)$.\n",
    "\n",
    "    - If a Bernoulli random variable is constructed as an indicator for the event $E$, then the success probability is the chance of the associated event, $p = \\text{Pr}(E)$.\n",
    "\n",
    "    - This language is arbitrary since the encoding to 0 and 1 is arbitrary. We could just as well call 0 success and 1 failure, but the assignment of 1 to success is so standard we will accept it as convention.\n",
    "\n",
    "Experiment with the code cell below to visualize Bernoulli distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, Bernoulli random variables:\n",
    "\n",
    "1. Illustrate the two general approaches to defining a distribution. Either:\n",
    "\n",
    "    - Write down a process that produces the variable (e.g. I am interested in some event, and assign it an indicator $X$)\n",
    "\n",
    "    - Or, fix the support and distribution function for the random variable. \n",
    "    \n",
    "1. Many random variables are specified by fixing a free **parameter**. The Bernoulli has only one free parameter, the success probability $p$. Usually the parameters are either variables that can be toggled to change the shape of the distribution, so that it can be adapted to model the scenario of interest, or, are free variables involved in the definition of the process that produces the variable (e.g. the probability of the event that $X$ indicates).\n",
    "\n",
    "1. Are a useful building block for other discrete disctributions.\n",
    "\n",
    "Let's build some other discrete distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Random Variables\n",
    "\n",
    "Suppose that you start flipping a coin. You are waiting to see a heads. Let $W$ denote the number of flips up to and including the flip when you see your first head. How is $W$ distributed?\n",
    "\n",
    "As always, start with the support. The random variable $W$ is a count, so it must be integer valued. In order to see a head, we must flip at least once, so $W \\geq 1$. \n",
    "\n",
    "Must $W$ be smaller than any upper bound $w \\geq 0$? For instance, must $W \\leq 10$?\n",
    "\n",
    "No. While unlikely, it is possible to see ten tails in ten tosses. So, $W$ could be greater than 10. What about 20? The same logic applies. 50? The same logic applies.\n",
    "\n",
    "No matter what $w$ you pick, it is possible, if very improbable, that $W > w$. \n",
    "\n",
    "This means that $W$ is unbounded above. In other words, the random variable $W$ is supported on the set of all positive integers $\\{1,2,3,...\\}$. This is our first example of a random variable that can take on infinitely many values. \n",
    "\n",
    "The fact that $W$ can take on infinitely many different values may be disturbing. So far all of our outcome spaces have contained finitely many outcomes. Do not fear. There is no issue extending to infinitely many outcomes. We just have to obey the axioms. In particular, we need to make sure our chance model, or PMF, assigns chances such that:\n",
    "\n",
    "$$\\sum_{w = 1}^{\\infty} \\text{PMF}(w) = \\text{Pr}(W = 1) + \\text{Pr}(W = 2) + \\text{Pr}(W = 3) + \\text{Pr}(W = 4) + ... = 1.$$\n",
    "\n",
    "If the sum equals 1, then the distribution is normalized, so is valid. \n",
    "\n",
    ":::{caution} A Note on Infinities\n",
    ":class: dropdown\n",
    "\n",
    "Not all infinities are alike. Some infinities are larger than others. \n",
    "\n",
    "The set of all nonnegative integers is a **countable** infinity (or, **countably infinite**) since we can enumerate all the integers, place them in a list, and, if we started counting, we would reach any integer after a finite number of elements in the list. Just count in order:\n",
    "\n",
    "$$ 0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow ...$$\n",
    "\n",
    "After $n$ steps you will reach the integer $n$, so every integer, $m$ will be reached in $m < \\infty$ steps.\n",
    "\n",
    "The set of all integers is also countably infinite. Count:\n",
    "\n",
    "$$ 0 \\rightarrow 1 \\rightarrow -1 \\rightarrow 2 \\rightarrow -2 \\rightarrow 3 \\rightarrow -3...$$\n",
    "\n",
    "You should convince yourself that this scheme will reach any integer eventually. \n",
    "\n",
    "A rational number is any number that can be expressed as a ratio of all integers. It seems like the set of rational numbers should be much larger than the set of all integers since there are infinitely many rational numbers between every pair of integers. It turns out that the rationals are also countable, since there exists a scheme for listing every rational number in order, starting from 0!\n",
    "\n",
    "In contrast, the set of real numbers (any number you can write as a decimal) is not countably infinite. It is **uncountably infinite**. There is no way to list all of the real numbers in order. We will see that, while there are no definitional problems writing down distributions for random variables with countably infinite support, uncountable infinities pose a subtler problem.\n",
    "\n",
    ":::\n",
    "\n",
    "Now that we've established the support for $W$, we should find one of its distribution functions. Since it is the most natural, let's try and solve for its PMF.\n",
    "\n",
    "To find the PMF, ask the question, what is the chance that $W = w$?\n",
    "\n",
    "In order for the event $W = w$ to occur, we must fail on the first $w - 1$ tosses, and succeed on toss $w$. So, we express the event $W = w$ as an intersection of $w$ events:\n",
    "\n",
    "$$\\{W = w\\} = \\{\\text{fail on toss 1}\\} \\cap \\{\\text{fail on toss 2}\\} \\cap ...  \\{\\text{fail on toss } w - 1\\} \\cap \\{\\text{succeed on toss } w\\}$$\n",
    "\n",
    "Since you are flipping a coin, the outcome of any toss is independent of the outcome of any other toss. Therefore, we can compute the chance of this joint event using the multiplication rule:\n",
    "\n",
    "$$\\text{Pr}(W = w) = \\text{Pr}(\\text{fail on toss 1}) \\times \\text{Pr}(\\text{fail on toss 2}) \\times ... \\text{Pr}(\\text{fail on toss } w - 1) \\times \\text{Pr}(\\text{succeed on toss } w)$$\n",
    "\n",
    "Notice, each toss is a binary event. It has two possible outcomes: heads (success), or tails (failure). So, we can represent each toss with a Bernoulli random variable. The Bernoulli has som success probability $p$ and failure probbility $q = 1 - p$.\n",
    "\n",
    "Since I am tossing the same coin repeatedly, the chance I fail or succeed on any particular toss is the same as any other toss. Therefore:\n",
    "\n",
    "$$\\text{Pr}(W = w) = q \\times q \\times ... \\times q \\times p = q^{w-1} p = (1 - p)^{w-1} p$$\n",
    "\n",
    "You can read the equation above as, the chance I fail $w-1$ times ($q^{w-1}$) times the chance I succeed on the last trial ($p$). \n",
    "\n",
    "For a fair coin, $q = p = 1/2$, so:\n",
    "\n",
    "$$\\text{Pr}(W = w) = (1 - p)^{w-1} p = \\left(\\frac{1}{2}\\right)^w$$\n",
    "\n",
    "We've plotted the PMF below:\n",
    "\n",
    "![Geometric PMF.](geometric_pmf_barplot.svg \"Geometric PMF.\")\n",
    "\n",
    "Notice that, each bar is $1/2$ the height of the preceeding bar, since, on each toss, there is a $1/2$ chance of success. \n",
    "\n",
    ":::{tip} Exercise ðŸ› ï¸\n",
    "\n",
    "Try to visualize the associated CDF. What should it look like? What is its value at $w = 1$? How does it behave as $w$ increases?\n",
    "\n",
    "Note: these questions can be answered entirely by reasoning without formulas.\n",
    ":::\n",
    "\n",
    ":::{hint} Solution\n",
    ":class: dropdown\n",
    "\n",
    "![Geometric CDF.](geometric_cdf_barplot.svg \"Geometric CDF.\")\n",
    "\n",
    "The CDF starts at $1/2$, since the chance of succeeding in 1 or fewer tosses is the chance of succeeding on the first toss, and increases with each subsequent toss since, the chance of succeeding at least once in $w$ tosses grows as we allow more tosses. The CDF approaches 1 since, the chance of no successes in $w$ tosses approaches zero for large $w$.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variable $W$ is an example of a **geometric** random variable:\n",
    "\n",
    ":::{note} Geometric Random Variable Defintion (Explicit)\n",
    "\n",
    "A **geometric random variable** $W$, drawn with success probability $p$, is:\n",
    "\n",
    "1. supported on all nonnegative integers: $W \\in \\{1,2,3,...\\}$\n",
    "1. has PMF of the form:\n",
    "\n",
    "$$\\text{PMF}(w) = \\text{Pr}(W = w) = (1 - p)^{w-1} p $$\n",
    "\n",
    ":::\n",
    "\n",
    "We denote the sentence, *the random variable $W$ is drawn from a geometric distribution with parameter $p$* with:\n",
    "\n",
    "$$W \\sim \\text{Geom}(p) $$\n",
    "\n",
    "Like Bernoulli random variables, Geometric random variables have one free parameter $p$ which must be between 0 and 1.\n",
    "\n",
    "The definition provided above is explicit. It defines the family of random variables by fixing a support (range of possible values), and form for the PMF. \n",
    "\n",
    "We can also define geoemtric random variables implicitly, by identifying the types of processes that produce geometric random variables:\n",
    "\n",
    ":::{note} Geometric Random Variable Defintion (Implicit)\n",
    "\n",
    "If $W$ is the number of trials up to, and including, the first success, in a sequence of independent, indentical Bernoulli trials with success probability $p$, then $W \\sim \\text{Geom}(p)$.\n",
    "\n",
    ":::\n",
    "\n",
    "In other words, geometric random variables describe the number of attempts needed until a first success in a sequence of independent, identical, binary experiments. \n",
    "\n",
    ":::{tip} Example\n",
    "\n",
    "You roll a die repeatedly until you see a 4 or 6. Let $R$ be the number of rolls up to, and including, the first roll when you see a 4 or a 6. How is $R$ distributed?\n",
    "\n",
    "First, recognize the process. $R$ is a random count, counting the number of trials in a sequence of independent and identical experiments. These experiments have only two outcomes. The roll is a four or a size, or the roll is neither a 4 nor a 6. Therefore, each experiment has binary outcomes, that can be encoded as indicators, so are equivalent to Bernoulli trials. It follows that $R$ must be a geometric random variable.\n",
    "\n",
    "Geometric random variables are specified by a success probability, $p$. The success probability $p$ is the chance any particular trial succeeds. To find $p$, we need the probability that a die lands on a 4 or a 6. If the die is fair, then this chance is $2/6 = 1/3$. Therefore:\n",
    "\n",
    "$$R \\sim \\text{Geom}(1/3). $$\n",
    "\n",
    "Therefore, the random variable $R$ has PMF:\n",
    "\n",
    "$$\\text{PMF}(r) = \\text{Pr}(R = r) = \\left(1 - \\frac{1}{3} \\right)^{r-1} \\times \\frac{1}{3} = \\left( \\frac{2}{3} \\right)^{r-1} \\frac{1}{3} .$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "Experiment with the code cell below to visualize Geometric distributions. Select \"Discrete\" then \"Geometric\" from the drop down. Adjust the parameter $p$ until you are confident you could explain how the shape of the PMF depends on $p$. Try to explain, by interpreting $p$ as the chance of success in a string of independent, identical trials, why the shape depends on $p$ in the way you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Random Variables\n",
    "\n",
    "Let's construct a random count in a different way.\n",
    "\n",
    "Suppose you flip a coin 10 times. What is the chance that, in 10 tosses, you see 7 heads?\n",
    "\n",
    "Let $X$ denote the number of heads. As always, you should start by asking, *what is the range of possible values of $X$?\n",
    "\n",
    "- $X$ is a count so it must be integer-valued.\n",
    "\n",
    "- At least, all of the tosses are tails, so $X$ could be as small as 0.\n",
    "\n",
    "- At most, all of the tosses are heads, so $X$ could be as large as 10.\n",
    "\n",
    "Therefore, $X \\in \\{0,1,2,3,...,10\\}$. In other words, $X$ is supported on all whole numbers between 0 and 10.\n",
    "\n",
    "Next, let's derive the PMF. To find the PMF, first ask a question of the kind, *what is $\\text{Pr}(X = x)$?* Picka value $x$ that is in the support. Solve for the chance, then substitute for generic $x$. For example, what's the chance that $X = 7$?\n",
    "\n",
    "The ebvent $X = 7$ can happen in a few different ways. As usual, we'll start by partitioning into all the ways the event can occur. \n",
    "\n",
    "Here's one $HHHHHHHTTT$. Here's another $TTTHHHHHHH$. And here's a third $THHHTTHHHH$. All of these strings of tosses contain 7 heads and 3 tails. \n",
    "\n",
    "What's the chance we see the string $HHHHHHHTTT$?\n",
    "\n",
    "This string is a sequence of events, so its a series of *and* statements. The outcome of your 5th toss has nothing to do with the outcome of your 7th toss, so the tosses are independent. Therefore, we can use the multiplication rule for independent events to split up each chance:\n",
    "\n",
    "$$\\begin{aligned} \\text{Pr}(HHHHHHHTTT) & = \\text{Pr}(H \\text{ and } H \\text{ and } ... H \\text{ and } T \\text{ and } T \\text{ and } T) \\\\\n",
    "& = \\text{Pr}(H) \\times \\text{Pr}(H) \\times ... \\text{Pr}(H) \\times \\text{Pr}(T) \\times \\text{Pr}(T) \\times \\text{Pr}(T) \\end{aligned}$$\n",
    "\n",
    "We could, at this point, plu in $1/2$ for all the chances, but it will be more informative to keep the chance of heads and tails separate. So, let's use $p$ for the chance of a heads (success) and $q = 1 - p$ for the chance of a tails (failure). Then:\n",
    "\n",
    "$$\\text{Pr}(HHHHHHHTTT) = p \\times p \\times ... \\times p \\times q \\times q \\times q = p^7 q^3.$$\n",
    "\n",
    "This equation is suggestive. Notice:\n",
    "\n",
    "1. The answer on the right hand side does not depend on the order of the heads and tails in the string defining the event, only the number of heads and tails. This is sensible. If we run a string of independent, identical experiments, then the chance of any sequence of outcomes should only depend on the number of times we saw each outcome, not their order. So, we can leave the right hand side alone, and replace the string $HHHHHHHTTT$ with any string of 10 characters, 7 of which are heads, and 3 of which are tails. We'll need to count how many different strings satisfy this rule in a moment. \n",
    "\n",
    "1. Each term has a clear source in the original question. We've raised $p$ to the 7th power since we asked about the chance of $7$ heads. Since we are counting the number of heads with $X$, the chance $X = 7$ is the chance $X$ takes on a particular selection drawn from its possible values, $x$. So, anytime we ask a question, *what is the chance we see $X = x$ heads*, we should expect to see $p$ raised to the $x$. \n",
    "\n",
    "1. We've raised $q$ to the third power since, to see 7 heads in 10 tosses, we must see 3 tails. If we'd asked, *what is the chance we see $X = x$ heads*, we are equivalently asking, *what's the chance we see $n - x$ tails$ where $n = 10$ was the number of tosses. So, we can rewrite the right hand side of the equation: \n",
    "\n",
    "$$p^{x}q^{n-x} = p^x (1 - p)^{n-x}.$$ \n",
    "\n",
    "What we've just done is solved a specific case, then generalized the result to work for variants of our original question. Notice that, if we weren't tossing a fair coin, but repeating some other binary experiment, then we could choose any success probability $p \\in [0,1]$. \n",
    "\n",
    "Now, we've solved for the chance of seeing any specific string of $x$ successes and $n - x$ failures in a sequence of independent, identical, Bernoulli trials, each which succeeds with chance $p$. We can put these together with the addition rule (marginalize) to recover the probability of seeing $x$ sucesses and $n - x$ falures:\n",
    "\n",
    "$$\\text{Pr}(x \\text{ successes and } n - x \\text{ failures}) = \\sum_{...} p^x (1 - p)^{n-x} $$\n",
    "\n",
    "where the $...$ stands for summing over all sequences of length $n$, containing $x$ successes and $n - x$ failures. We've left this as a ... for now since it is too long for a subscript, and, since the next step won't depend on the details of ...\n",
    "\n",
    "Take a look at the sum again. The term inside the sum does not depend on the sequence chosen. So, it can be taken outside. Then the sum can be written:\n",
    "\n",
    "$$\\text{Pr}(x \\text{ successes and } n - x \\text{ failures}) = \\left(\\sum_{...} 1 \\right) p^x (1 - p)^{n-x} $$\n",
    "\n",
    "The sum of any constant $m$ times is $m \\times \\text{the constant}$. The sum of the number 1 $m$ times is just $m$. Therefore:\n",
    "\n",
    "$$\\text{Pr}(x \\text{ successes and } n - x \\text{ failures}) = |\\text{all distinct sequences of }x \\text{ successes and } n - x \\text{ failures}| \\times p^x (1 - p)^{n - x}.$$\n",
    "\n",
    "In other words, *the chance of $x$ successes in $n$ independent, identical, binary trials, is the number of ways of ordering $x$ successes and $n - x$ failures, times the chance of any particular sequence of $x$ successes and $n - x$ failures.*\n",
    "\n",
    "All that is left to do is to count the number of distinct ways we can arrange $x$ successes and $n - x$ failures. \n",
    "\n",
    ":::{tip} Exercise ðŸ› ï¸\n",
    "\n",
    "Try listing all the ways of arranging 4 heads and 2 tails. Count the number of arrangements.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{hint} Solution\n",
    ":class: dropdown\n",
    "$$\\begin{aligned} \\{& HHHHTT, \\\\ & HHHTHT,HHHTTH, \\\\\n",
    "& HHTHHT,HHTHTH,HHTTHH,\\\\\n",
    "& HTHHHT,HTHHTH,HTHTHH,HTTHHH,\\\\\n",
    "& THHHHT,THHHTH,THHTHH,THTHHH,TTHHHH\\}  \\end{aligned}$$\n",
    "\n",
    "So, there are 15 distinct ways to arrange 4 heads and 2 tails.\n",
    ":::\n",
    "\n",
    "Clearly, listing all the arrangements is too cumbersome for a long string. Even listing all the arrangements of 7 heads in 10 tosses would require listing 120 different arrangemenets.\n",
    "\n",
    "So, we use combinatorics instead. If you haven't solved this type of problem before, or its been a while, check [Appendix A.1](chA.1).\n",
    "\n",
    "There are $n!$ distinct arrangements of $n$ distinguishable outcomes. If the outcomes are divided into two sets of sizes $x$ and $n - x$, then we've overcounted by the $x!$ ways of rearranging the elements of the first set amongst themselves, and the $(n - x)!$ ways of rearranging the elements of the second set. So, the correct count is:\n",
    "\n",
    "$$\\text{number of arrangements} = \\frac{n!}{x!(n-x)!} = \\left(\\begin{array}{c} n \\\\ x \\end{array} \\right) $$\n",
    "\n",
    "We call the expression above a **binomial** or **choose coefficient**. We read it $n$ *choose* $x$. The *binomial* distribution get's its name from this coefficient.\n",
    "\n",
    ":::{important} Counting Sequences Via an Outcome Tree\n",
    ":class: dropdown\n",
    "\n",
    "This argument is a work in progress. Check back in.\n",
    ":::\n",
    "\n",
    "We now have all the pieces we needed to compute the desired chance. The chance of $x$ successes in $n$ identical, independent, binary trials is:\n",
    "\n",
    "$$\\text{Pr}(X = x) = \\left(\\begin{array}{c} n \\\\ x \\end{array} \\right) p^x (1- p)^{n-x}. $$\n",
    "\n",
    ":::{tip} Example\n",
    "\n",
    "So, the chance of 7 heads in a string of 10 tosses of a fair coin is:\n",
    "\n",
    "$$\\begin{aligned} \\text{Pr}(X = x) & = \\left(\\begin{array}{c} 10 \\\\ 7 \\end{array} \\right) 0.5^7 (1- 0.5)^{10-7} \\\\\n",
    "& = \\frac{10!}{7! 3!} 0.5^{10} = \\frac{10 \\times 9 \\times 8}{3 \\times 2 \\times 1} 0.5^{10} \\\\ & = \\frac{10 \\times 3 \\times 4}{1} 0.5^{10} = 120 \\times 0.5^{10} \\\\\n",
    "& = \\frac{120}{1024} \\approx 0.1 \\end{aligned}$$\n",
    "\n",
    "So, there is about a 10% chance to see 7 heads in 10 tosses.\n",
    ":::\n",
    "\n",
    "The random variable, $X$, is an example of a *binomial* random variable. We can define it explicitly or implicitly.\n",
    "\n",
    ":::{note} Binomial Random Variable Definition (Explicit)\n",
    "\n",
    "A **binomial random variable** $X$, drawn with parameters $n$ and $p$, is:\n",
    "\n",
    "1. supported on the set $X \\in \\{0,1,2,...n\\}$, and\n",
    "1. has PMF of the form:\n",
    "\n",
    "$$\\text{PMF}(x) = \\text{Pr}(X = x) = \\left(\\begin{array}{c} n \\\\ x \\end{array} \\right) p^x (1- p)^{n-x}. $$\n",
    ":::\n",
    "\n",
    ":::{note} Binomial Random Variable Definition (Implicit)\n",
    "\n",
    "Any random variable that can be constructed as the total number of successes in a string of $n$ identical, independent, Bernoulli (binary) trials, each with success probability $p$, is a **binomial random variable** with parameters $n$ and $p$.\n",
    "\n",
    ":::\n",
    "\n",
    "We denote the statement *$X$ is a binomial random variable on $n$ trials with success probability $p$*, with:\n",
    "\n",
    "$$X \\sim \\text{Binom}(n,p). $$\n",
    "\n",
    "\n",
    "Experiment with the code cell below to visualize Binomial distributions. Adjust the parameters $n$ and $p$ until you are confident you could explain how the shape of the PMF depends both. Try to explain, by interpreting $n$ and $p$, why the shape depends on $p$ in the way you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other distributions:\n",
    "\n",
    "There are many discrete distribution models, each associated with a different family of distribution functions, and a different collection of generating processes. We've built a demo that you can use to explore some of the most important discrete distributions. You won't need to know these all in detail, but this demo might be useful for you in future classes. We will ask you to interact with it on your HW. \n",
    "\n",
    "If you're interested in any of the other distributions, come ask the Professor. Given time, we'll tough briefly on the Poisson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some General Lessons\n",
    "\n",
    "Here are some last thoughts:\n",
    "\n",
    "1. It's worth remembering that there are two ways of defining a random variable, and associated distribution. \n",
    "    - The implicit approach is the most natural from an applied perspective. We're interested in some random process in the world, have a natural language description of the process, and want to derive, from that description, the chances of events. Most of the important discrete distributions are tied to a particular story (natural language description of process), family of related stories. This approach is less common for distributions associated with continuous variables. \n",
    "    - All distributions can also be defined explicitly. Just like we can define a set in natural language (implicitly), then convert to an explicit list of elements, we can also define a distribution explicitly, by listing all the possible values of the random variable, then writing down a function that computes the chance of each event. Usually these functions have free parameters that are:\n",
    "        - allowed to vary so that the modeler has the freedom to express many different distributions with a single function, and\n",
    "        - allowed to vary to represent natural variants in the story associated with the distribution. For example, the parameters $n$ and $p$ of the Binomial distribution are naturally associated with a number of trials, and a chance of success per trial.\n",
    "\n",
    "1. So, one skill you should develop is the ability to go back and forth between a functional form, with free parameters, and the shape of a distribution. It is very important that you understand how a distribution changes when we change its parameters. Understanding the parameteric dependence of chance means that you understand how the free elements of a question (e.g. the number of trials), influence the answer to your question. We'll practice this skill explicitly in the next chapter. The better you get at it, the faster you'll be able to make sense of equations, see how your answers depend on the set up to a question, gut check whether your answers are sensible, and, if asked to develop a new model, to choose a family of functions. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
