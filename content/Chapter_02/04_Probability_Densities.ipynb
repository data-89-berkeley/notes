{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering:\n",
    "  title:\n",
    "    offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2.4)=\n",
    "# Probability Densities\n",
    "\n",
    "In [Section 2.3](ch2.3) we showed that, for any continuous random variable $X$, the chance $X$ equals some exact value, $x$, is zero, no matter which $x$ we choose. As a result, the PMF is zero everywhere:\n",
    "\n",
    "$$\\text{PMF}(x) = \\text{Pr}(X = x) = 0.$$\n",
    "\n",
    "This tells us nothing about the random variable except that it is continuous. It does not tell us how to compute chances. \n",
    "\n",
    "In this chapter we will introduce our last type of distribution function, a probability *density* function. Density functions are the continuous analog to mass functions for discrete random variables. If you are asked to picture a distribution in your head, and you picture a bell-curve, or a bell-curve shaped histogram whose bars could be made very narrow, then you are picturing a density function.\n",
    "\n",
    "This section will introduce density functions by experimentation with histograms for continuous random variables. Once we see that density is a natural idea for continuous random variables, we'll show how to relate densities to chances, then introduce some examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density\n",
    "\n",
    "### By Experiment\n",
    "\n",
    "Let's start with an experiment. We've already seen that we can:\n",
    "\n",
    "1. Define uniform continuous variables by equating chances to proportions of length or area\n",
    "1. Relate chances of events to long run frequencies\n",
    "1. Define arbitrary continuous random variables by fixing a cumulative distribution function (CDF).\n",
    "\n",
    "Let's set up an experiment that puts these ideas to work. We'll start with a uniform continuous random variable, transform it to get something non-uniform, divide up its support into small pieces, then draw many copies of the variable and see how frequently it lands in each piece. Since probabilities equal long run frequencies, the associated histogram will represent a distribution function. Formally, its a categorical distribution with categories equal to the histogram bins. The key continuous idea is, *we have the freedom to change the width of the bins*.\n",
    "\n",
    "To get a detailed picture of the distribution, we'll try to take a limit where the bins get arbitrarily small. Per [Section 2.3](ch2.3), we'll see that, making the bins narrower, lowers the frequency that outcomes land in each bin, so to keep the histogram bars about the same height as we narrow the bins we'll have to scale by the width of the bins. Scaling by width produces a density. Once we've seen a density, we'll show that, unlike the PMF, the density function can be used to recover a CDF. Then, since a CDF specifies a measure, the density function is enough to define a probability model. \n",
    "\n",
    "Imagine you are throwing darts at a dartboard. We'll imagine that you're not very good at darts, so the position of each dart is uniform over the board. For a circular board this means we are picking a location uniformly from the interior of a circle. \n",
    "\n",
    "This is a uniform continuous model, so we can measure chances. The chance a dart lands in some region $A$ on the board, is:\n",
    "\n",
    "$$\\text{Pr}(\\text{dart lands in } A) = \\frac{\\text{area of } A}{\\text{area of dart board}} = \\frac{|A|}{|\\Omega|} $$\n",
    "\n",
    "where $\\Omega$ is the collection of all possible positions the dart could land (the circular board). \n",
    "\n",
    "You want your darts to land near the center of the board. So, you decide to measure the distance the dart lands from the center. The dart's position is random, so is it's distance from the middle of the board. We'll call that random variable $R$ for radius. \n",
    "\n",
    "$R$ is a continuous random variable. For simplicity, let's assume the dart board has radius 1. Then, $R$ is supported on the interval $[0,1]$ since we, at best, hit the center of the board, and, at worst, hit the outer edge. \n",
    "\n",
    "Is $R$ uniformly distributed? Take a moment to think about this carefully before answering. \n",
    "\n",
    "If you're unsure how to proceed, remember that its often easier to start with a CDF. What is:\n",
    "\n",
    "$$\\text{CDF}(r) = \\text{Pr}(R \\leq r)? $$\n",
    "\n",
    "Well, the region on the board where $R \\leq r$ is the interior of a circle centered at zero, with radius $0 \\leq r \\leq 1$. We don't have to worry about the distinction between $\\leq r$ and $< r$ since the position of the dart is continuous. We can now visualize the event $R \\leq r$ as a filled circle with radius $r$ inside a larger circle of radius 1. The chance we land in the inner circle is the ratio of its area to the area of the full circle.\n",
    "\n",
    "So, using probability by proportion:\n",
    "\n",
    "$$\\text{CDF}(r) = \\text{Pr}(R \\leq r) = \\frac{\\pi r^2}{\\pi 1^2} = r^2. $$\n",
    "\n",
    "To check whether this CDF could correspond to a uniform measure, let's compare the chances that $R$ lands in two intervals of equal length. For instance, what are the odds that $R > 1/2$ compared to the odds $R \\leq 1/2$?\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\text{Pr}(R > 0.5)}{\\text{Pr}(R \\leq 0.5)} = \\frac{1 - \\text{Pr}(R \\leq 0.5)}{\\text{Pr}(R \\leq 0.5)} \\\\& = \\frac{1 - \\text{CDF}(0.5)}{\\text{CDF}(0.5)} \\\\& = \\frac{1 - 0.5^2}{0.5^2} \\\\& = 4 \\times \\frac{3}{4} \\\\& = 3 \\end{aligned}$$\n",
    "\n",
    "So, the dart is *3 times more likely* to land in the outer interval, $(1/2,1]$ than the inner interval, $[0,1/2]$. So, the dart's distance from the origin cannot be uniformly distributed. It is more likely to land farther from the center of the board than closer to the center. \n",
    "\n",
    "To visualize this bias, let's run the following experiment:\n",
    "\n",
    "1. Throw a bunch of darts (sample uniformly from the circle), \n",
    "1. Compute the distance of each dart from the center (sample $R$ repeatedly), then\n",
    "1. Divide the interval $[0,1]$ into many small pieces, count the frequency with which $R$ lands in each segment, and\n",
    "1. Plot the associated histogram.\n",
    "\n",
    "We'll let $\\Delta r$ denote the width of the histogram bins. Remember, we get to choose these. They are an artifact of our visualization scheme. To get a precise picture, we will want to both, take many samples to eliminate randomness in the bar heights, and, make the bins very narrow. \n",
    "\n",
    "You can run this experiment yourself using the demo below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you draw enough samples, and keep the bins small enough for a detailed plot, but big enough that they each contain a lot of samples, then you should get a pretty clear picture. The histogram should look something like this:\n",
    "\n",
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!\n",
    "\n",
    "It's basically a linear wedge. A linear trend is not surprising, since running sums act like integrals, the integral of a linear function is quadratic, and we saw that the CDF is a quadratic function of $r$. \n",
    "\n",
    "So, it's natural to think that, the distribution of $R$ should be described by some linear function of $r$. To recover that function, we need to make sure our histogram plot is not sensitive to arbitrary choices we made when we set up the experiment. \n",
    "\n",
    "There were two free parameters in the set up:\n",
    "\n",
    "1. The number of samples, and\n",
    "1. The bin widths, $\\Delta r$.\n",
    "\n",
    "To see why these are problematic, open the demo again, and set the vertical heights of the bars equal to the raw number of samples that land in each interval. This is the simplest histogram convention. The height of each bar is the number of times the corresponding event occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try varying the number of samples. If you increase the number of samples to make the plot less noisy, you should see that all the bars get taller. The histogram is still roughly linear in $r$ but its slope changed. \n",
    "\n",
    "The distribution of $R$ was fixed by the sampling process, so cannot depend on the total number of samples. Accordingly, we can't equate probability to a raw count of occurences. That's not surprising, probabilities should match long run frequencies.\n",
    "\n",
    "So, to make sure that our plot is invariant to (does not depend on) the total number of samples, we should set the height of the bars to the *frequency* with which each event occured. This converts to plotting chances rather than counts.\n",
    "\n",
    "Set the y-axis back to frequency and try varying the sample size. You should see that, as long as you keep the bin widths fixed, the histogram now converges to a fixed function in the limit of many samples. This function is a categorical distribution on the bins. It is analogous to a PMF when we round $R$ to some fixed, finite, precision.\n",
    "\n",
    "So far so good. But our plot could still depend on the bin widths. \n",
    "\n",
    "Keep plotting frequency, and try varying the bin width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that, if you make the bins wider, not too much changes except the bars get taller, but, as you make the bins smaller, things start to fall apart. The narrower the bins, the shorter the bars, and the noiser the pattern. The second effect, the amount of noise, can be fixed by increasing the sample size. \n",
    "\n",
    "So, take a very large sample size, and see how small you can set the bars. Can you set the bars small enough so that their height stops changing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No!\n",
    "\n",
    "Everytime you shrink the bins, the bars get shorter. The function we're looking for is still linear, but its slope depends on the arbitrarily chosen bin width. Worse, its slope approaches zero as we make the bins narrow. \n",
    "\n",
    "That last observation is the main result of [Section 2.3](ch2.3). If a random variable is continuous, then the chance it lands in a shrinking series of intervals decreases as the intervals get narrower, and approaches zero as the intervals converge to a point. The distance from the center of the board, $R$, is a *continuous* random variable, so the chance $R$ lands exactly in any very narrow bin must be very small. What we're seeing is experimental evidence that, for a continuous random variable, the chance of every exact event is zero, so its PMF is zero everywhere.\n",
    "\n",
    "Ok, what next?\n",
    "\n",
    "Well since the heights of the bars decrease as we make the bars narrow we could try an old trick from calculus. Let's scale the heights of the bars by dividing each frequency by the width of the associated bin. Then we're plotting frequency per bin width. Hopefully, dividing by the shrinking bin width should cancel out the decrease in frequency. \n",
    "\n",
    "Try it! Set the y-axis convention to frequency per width (density)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should see that, as long as you keep increasing the sample size, the histogram converges to the linear function: $f(r) = 2 r$. This function stays stable no matter how you vary the bin widths, or the sample sizes, as long as we have enough samples to average away noise in the frequencies!\n",
    "\n",
    "To check, lock the bin width $\\Delta x$ to a function that decreases in sample size, $n$, so that the bin widths approach zero as the sample size diverges, and do so slowly enough so that the number of samples in each bin increases as $n$ increases. \n",
    "\n",
    "Click the \"lock\" checkbox, then gradually increase the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've just recovered is a probability *density* function. We call it a density by analogy to mass per volume. \n",
    "\n",
    "In physics, the density of a region is the ratio of its mass to its volume. We can define the density of a point by contracting a sequence of regions near the point onto the point. For instance, the density of a point $x$ is the total mass within $\\Delta x$ of $x$, divided by the volume of the region of points within $\\Delta x$ of $x$, in the limit as $\\Delta x$ goes to zero. We call frequency per length a density since it has units of *probability mass per length*. This definition extends naturally to higher dimensions. We can define densities as *probabilities per unit area* or *probabilities per unit volume.*\n",
    "\n",
    "Working with densities will help us resolve one of the paradoxes from the end of [Section 2.3](ch2.3). There we saw that, when we sample a continuous random variable we always get a specific outcome, even though every specific outcome has zero chance. Somehow, all of the possible outcomes had zero chance, but collections of outcomes had nonzero chances. \n",
    "\n",
    "Density and mass behave the same way. The total mass in some region vanishes as we make the region arbitrarily small. So, the total mass at any point is zero. Yet, regions are composed of points, and can have nonzero mass. \n",
    "\n",
    "This paradox is resolved by integration. The total mass of an object is not the sum of the masses of every infinitesimally small piece of the object. It is the integral of the density of every point in the object. \n",
    "\n",
    "The same thing will be true for probability densities. Let's check it for our example. \n",
    "\n",
    "We'd proved that:\n",
    "\n",
    "$$\\text{CDF}(r) = \\text{Pr}(R \\leq r) = r^2 $$\n",
    "\n",
    "using probability by proportion. \n",
    "\n",
    "Then, by experiment, we'd guessed a density function:\n",
    "\n",
    "$$f(r) = 2 r. $$\n",
    "\n",
    "The region $R \\leq r$ is the interval $[0,r]$ since $R$ is a radius. Therefore, we should try integrating the density from 0 to $r$:\n",
    "\n",
    "$$\\int_{s = 0}^r f(s) ds = \\int_{s = 0}^r 2 s ds = s^2|_{s = 0}^{r} = r^2 - 0 = r^2. $$\n",
    "\n",
    "Therefore, in this example probability mass (e.g. chances) are related to densities by integration:\n",
    "\n",
    "$$\\text{CDF}(r) = \\int_{s = 0}^r f(s) ds. $$\n",
    "\n",
    "We'll see that this is always true for densities, and that we can use this relation to define continuous random variables starting directly from density functions. This approach is more intuitive than starting from cumulative probabilities, since density functions look like histograms. CDF's don't. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition and Relation to Chance\n",
    "\n",
    "Let's formalize our construction.\n",
    "\n",
    ":::{note} Probability Density Function Definition\n",
    "\n",
    "Given a continuous random variable $X$, its **probability density function (PDF)**, evaluated at $x$, is:\n",
    "\n",
    "$$\\text{PDF}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{\\text{Pr}(X \\in x \\pm \\frac{1}{2}\\Delta x)}{\\Delta x}. $$\n",
    "\n",
    "Youn should read this equation: *the probability density at $x$ is the chance $X$ is near to $x$, relative to the length of the interval we used to define \"near\".*\n",
    "\n",
    ":::{caution} Check Units\n",
    "\n",
    "Probability densities do not have the same units as probabilities. Probabilities have the units of proportions, or frequencies. These are ratios of matched quantities (ratios of counts), so probabilities ar eunit less. \n",
    "\n",
    "Probability densities have units of *probability per unit length*. So, they will depend on how we measure length. \n",
    "\n",
    ":::\n",
    "\n",
    "It is very common to denote:\n",
    "\n",
    "1. The PDF of a continuous random variable, $X$, evaluated at $x$: $f_X(x)$. Here we use a subscript $X$ to remind us which random variable, and a little $f$ for function.\n",
    "1. The CDF of a continuous random variable, $X$, evaluated at $x$: $F_X(x)$. Here we use a subscript $X$ to remind us which random variable and use a capital $F$ for function. \n",
    "\n",
    "We use a lower case for the density and an upper case for the CDF since this notation matches the standard convention in calculus. We'll adopt this notation since it is so widely used, but frequently remind you that little $f$ is the PDF, while $F$ is the CDF.\n",
    "\n",
    ":::{caution} Density is *not* Mass\n",
    "\n",
    "It is *very* easy to conflate a PDF and a PMF. Their abbreviations are almost identical, and they both correspond to our intuitive picture for the limit of a histogram. Pay careful attention to the distinction between density and mass. They have different units. Here's a quick reference key:\n",
    "\n",
    "1. If $X$ is discrete, don't worry about density, just use mass.\n",
    "1. If $X$ is continuous:\n",
    "    - The PMF is zero everywhere, so mass is useless. Never use a PMF.\n",
    "    - The PDF captures our mental image. Use a density.\n",
    "    - If you use a density, check units. Densities have units probability *per length*.\n",
    "\n",
    ":::\n",
    "\n",
    "The PDF and PMF are easy to mix up since the PDF looks like a rescaled limit of a PMF, when we make our intervals very narrow. This idea is more than an annoying stumbling block. The relationship between PDF, and its approximation with a PMF, explains how we should use PDF's to compute chances.\n",
    "\n",
    ":::{note} Approximating Chances from Densities\n",
    "\n",
    "Rearranging the limiting statement in the definition of the PDF gives the approximation: *the chance $X$ is within $\\Delta x$ of $x$ is, about, the value of the density at $x$, multiplied by the length of the interval, $\\Delta x$.*\n",
    "\n",
    "$$\\text{Pr}(X \\in x \\pm \\frac{1}{2}\\Delta x) \\approx  \\text{PDF}(x) \\Delta x = f_X(x) \\Delta x$$\n",
    "\n",
    "This approximation converges in the limit as $\\Delta x$ approaches zero.\n",
    "\n",
    ":::\n",
    "\n",
    "We can use the approximation statement above to recover a formula for chances from densities. Just like we integrate densities to recover mass, we should integrate probability densities to recover probability mass:\n",
    "\n",
    ":::{note} Computing Chances by Integrating Densities\n",
    "\n",
    "Given a continuous random variable $X$ with PDF $f_X(\\cdot)$, the chance $X$ is contained in any interval $[a,b]$ is:\n",
    "\n",
    "$$\\text{Pr}(X \\in [a,b]) = \\int_{x = a}^b f_X(x) dx. $$\n",
    "\n",
    ":::\n",
    "\n",
    "If there is one formula to remember from this section, it's the one above. *To compute chances from densities, integrate.*\n",
    "\n",
    ":::{hint} Proof\n",
    ":class: dropdown\n",
    "\n",
    "The proof follows from the usual proof that a Riemann approximation converges to an integral. First, pick an interval, $[a,b]$. Then, cut it into $n$ segments of equal length, $|b - a|/n$. Set $\\Delta x = |b - a|/n$. Then, the collection of segments partition the interval, so we can use the additivity axiom:\n",
    "\n",
    "$$\\text{Pr}(X \\in [a,b]) = \\text{Pr}(X \\ in [a,a + \\Delta x]) + \\text{Pr}(X \\ in [a + \\Delta x,a + 2 \\Delta x]) + ... \\text{Pr}(X \\ in [b - \\Delta x,b]) = \\sum_{j=0}^{n-1} \\text{Pr}(X \\in [x + j \\Delta x, x + (j+1) \\Delta x]). $$\n",
    "\n",
    "Let $x_j = a + j  \\frac{1}{2}\\Delta x$. Then we can write the same statement a bit more succinctly:\n",
    "\n",
    "$$\\text{Pr}(X \\in [a,b]) = \\sum_{j=0}^{n-1} \\text{Pr}(X \\in x_j \\pm \\Delta x). $$\n",
    "\n",
    "Now, if we make $n$ large, $\\Delta x$ becomes small, so each term in the sum is asking for the chance that $X$ is in some small interval centered at a point $x_j \\in [a, b]$. Plugging in the approximation: *probability on small interval is about density times length of interval* gives the usual Riemann approximation to an integral. It's just the rectangle rule:\n",
    "\n",
    "$$\\text{Pr}(X \\in [a,b]) \\approx \\sum_{j=0}^{n-1} \\text{PDF}(x_j) \\Delta x = \\sum_{j=0}^{n-1} f_X(x_j) \\Delta x. $$\n",
    "\n",
    "To make the approximation exact, take $n$ to infinity. This sends $\\Delta x$ to $dx$, and replaces the sum with an integral:\n",
    "\n",
    "$$ \\text{Pr}(X \\in [a,b]) = \\int_{x = a}^b f_X(x) dx. $$\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Notice, the $\\Delta x$ involved in the definition in density is a reminder that to find chances we need to integrate, i.e. scale $f_X(x)$ by a small length, $dx$. It can be helpful to think that, anytime we go to use a density, we will need to integrate, so instead of thinking $f_X(x)$, always think $f_X(x) dx$.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "This equation justifies the famous \"area under the curve\" picture you may have seen for bell-curves. An integral computes an area under a curve. If we:\n",
    "\n",
    "1. Plot the PDF $f_x$\n",
    "1. Then find the area of a shaded region under the curve (integrate)\n",
    "\n",
    "we will have computed the chance the associated random variable lands in that interval. \n",
    "\n",
    ":::{important} Density Determines Measure\n",
    "Let's check that, if we define the support of a random variable, $X$, and a density function, $f_X$, then the measure constructed by integrating over the density is a valid probability measure. To do so, just check each axiom:\n",
    "\n",
    "1. **Nonnegativity:** $\\text{Pr}(X \\in [a,b]) = \\int_{x = a}^b f_X(x) dx$ must be greater than or equal to zero for all intervals $[a,b]$. This requires $f_X(x) \\geq 0$ for all $x$. \n",
    "    - So, to satisfy nonnegativity, $f_X$ must be a nonnegative function, $f_X(x) \\geq 0$.\n",
    "\n",
    "1. **Normalization:** $\\text{Pr}(X \\in (-\\infty,\\infty)) = \\int_{x = -\\infty}^{\\infty} f_X(x) dx$. Since any number is between negative and positive infinity, the integral must equal 1. \n",
    "    - So, to satisfy normalization, $f_X$ must integrate to one. *The area under the entire density curve must equal one.*\n",
    "\n",
    "1. **Additivity:** This is a good exercise to check your self. Is it true that, if $\\text{Pr}(X \\in [a,b]) = \\int_{x = a}^b f_X(x) dx$, then the probability $X$ lands in one of two disjoint intervals is the sum of the chances that $X$ lands in each interval?\n",
    "\n",
    ":::\n",
    "\n",
    "Therefore, a proposed function $f_X$ is a valid density function only if:\n",
    "\n",
    ":::{note} Rules for Density Functions\n",
    "A function $f(x)$ is a valid density function if and only if:\n",
    "\n",
    "1. $f(x) \\geq 0$ for all $x$... there is no such thing as negative density\n",
    "1. $\\int_{x = -\\infty}^{\\infty} f(x) dx = 1$... The density must be normalized, that is, integrate to one.\n",
    ":::\n",
    "\n",
    "### Working with Densities\n",
    "\n",
    "Now that we know what a density function is, and how to use it to compute chances, let's see how the density function is related to the CDF. Then, we'll make a table summarizing how to go between chances, densities, and cumulative probabilities. \n",
    "\n",
    "We already know how to compute the chance of an interval from a density:\n",
    "\n",
    "$$ \\text{Pr}(X \\in [a,b]) = \\int_{x = a}^b f_X(x) dx. $$\n",
    "\n",
    "The CDF is defined as the chance of a one-side interval. So, we get the CDF from the PDF by integrating:\n",
    "\n",
    ":::{note} CDF from PDF\n",
    "\n",
    "Given a continuous random variable $X$ with PDF $f_X(x)$, the CDF is recovered by:\n",
    "\n",
    "$$F_X(x) = \\text{CDF}(x)  = \\text{Pr}(X \\in (-\\infty,x]) = \\int_{s = \\infty}^x} f_X(s) ds.$$\n",
    "\n",
    ":::\n",
    "\n",
    "This equation is the continuous analog to the idea that a cumulative distribution is a running sum of a PMF. For continuous random variables, the CDF is the running *integral* of the *density*. Notice the two steps in this analogy. Replace a sum with an integral, then replace a mass function with a density function. In practice, this is the only real operational change you need to make to find probabilities for continuous random variables:\n",
    "\n",
    ":::{tip} Discrete to Continuous by Analogy\n",
    "\n",
    "If you would solve a discrete problem by *summing* over *probability masses*, then, for the matching continuous problem, *integrate* over *densities*. \n",
    "\n",
    ":::\n",
    "\n",
    "This equation also justifies the big $F$, little $f$ notation for cumulative probabilities and densities. The CDF is the integral of the PDF, or, is the *anti*-derivative of the PDF. The notation $f$ and $F$ for function and anti-derivative is the standard notation in calculus. You may remember it from your unit on the fundamental theorem of calculus. \n",
    "\n",
    "As always, a CDF can be used to find the chance $X$ lands in any interval. So, if we are given a PDF, integrate it to get a CDF, then we can take differences in CDF values to find chances:\n",
    "\n",
    "$$\\text{Pr}(X \\in [a,b]) = \\text{CDF}(b) - \\text{CDF}(a) = F_X(b) - F_X(a). $$\n",
    "\n",
    ":::{tip} Exercise ðŸ› ï¸\n",
    "\n",
    "Convince yourself that the statements:\n",
    "\n",
    "1. $\\text{CDF}(x) = F_X(x) = \\int_{-\\infty}^{x} f_X(s) ds$,\n",
    "1. $\\text{Pr}(X \\in [a,b]) = \\int_{a}^{b} f_X(s) ds$, and\n",
    "1. $\\text{Pr}(X \\in [a,b]) = \\text{CDF}(b) - \\text{CDF}(a)$\n",
    "\n",
    "are consistent with each other, and that, any two imply the third.\n",
    ":::\n",
    "\n",
    "What if we started from a CDF?\n",
    "\n",
    "Well, the CDF is the anti-derivative of the PDF, so the PDF is the derivative of the CDF! If the CDF is the integral (area under the curve) of the PDF, then the PDF must be the slope of the CDF. This is just the good old fundamental theorem of calculus.\n",
    "\n",
    ":::{note} PDF from CDF\n",
    "\n",
    "Given a continuous random variable $X$ with CDF $F_X(x)$, the random variable has PDF:\n",
    "\n",
    "$$\\text{PDF}(x) = f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx}\\text{CDF}(x) $$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{hint} Proof\n",
    ":class: dropdown\n",
    "\n",
    "Let's try to show this more directly (e.g. without invoking the fundamental theorem of calculus). Recall two equations:\n",
    "\n",
    "1. $\\text{PDF}(x) = f_X(x) = \\lim_{\\Delta x} \\frac{1}{\\Delta x} \\text{Pr}\\left(X \\in x \\pm \\frac{1}{2} \\Delta x \\right)$\n",
    "1. $\\text{Pr}(X \\in [a,b]) = \\text{CDF}(b) - \\text{CDF}(a) = F_X(a) - F_X(b)$.\n",
    "\n",
    "Then, putting the two together:\n",
    "\n",
    "$$\\text{PDF}(x) = f_X(x) = \\lim_{\\Delta x} \\frac{1}{\\Delta x} \\left(F_X(x + \\frac{1}{2} \\Delta x) - F_X(x - \\frac{1}{2} \\Delta x)\\right)$$\n",
    "\n",
    "The expression inside the limit is the slope of the secant line connecting $[x - \\frac{1}{2} \\Delta x, F_X(x - \\frac{1}{2} \\Delta x)]$ to $[x + \\frac{1}{2} \\Delta x, F_X(x + \\frac{1}{2} \\Delta x)]$ since it equals the change in $F$ divided by the change in $x$. The slope of a secant, in the limit as the two endpoints approach, is the definition of a derivative. Therefore:\n",
    "\n",
    "$$\\text{PDF}(x) = f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} \\text{CDF}(x). $$\n",
    "\n",
    ":::\n",
    "\n",
    "We can now move freely between density function, CDF, and measure. Given a rule for computing any of the three, we can solve for the other two. \n",
    "\n",
    "Here's a table summarizing the procedures. Memorize it. It's the most important part of this section. See if you can fill in the entries of the blank table below before opening the expandable section beneath the blank table. Fill in each \"?\" with a formula that recovers the object in the row header from the object in the column header. We've given an example in the first row.\n",
    "\n",
    "Object | PDF  | CDF  | Measure\n",
    ":----:|:-------------|:-------------|:-------------|\n",
    "PDF: $f_X(x)$  | . | **?**  | $\\lim_{\\Delta x \\rightarrow 0} \\frac{1}{\\Delta x} \\text{Pr}\\left(X \\in x \\pm \\frac{1}{2} \\Delta x \\right)$ |\n",
    "CDF: $F_X(x)$ | **?**  | . | **?**  |\n",
    "Measure: $\\text{Pr}(X \\in [a,b])$ | **?**  | **?** | . |\n",
    "\n",
    ":::{tip} Densities, Cumulative Distributions, and Measures\n",
    "\n",
    "Object | PDF  | CDF  | Measure\n",
    ":----:|:-------------|:-------------|:-------------|\n",
    "PDF: $f_X(x)$  | . | $\\frac{d}{dx} F_X(x)$  | $\\lim_{\\Delta x \\rightarrow 0} \\frac{1}{\\Delta x} \\text{Pr}\\left(X \\in x \\pm \\frac{1}{2} \\Delta x \\right)$ |\n",
    "CDF: $F_X(x)$ | $\\int_{s = -\\infty}^x f_X(s) ds$  | . | $\\text{Pr}(X \\in (-\\infty,x])$  |\n",
    "Measure: $\\text{Pr}(X \\in [a,b])$ | $\\int_{x = a}^b f_X(s) ds$  | $F_X(b) - F_X(a)$ | . |\n",
    "\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling by Shape\n",
    "\n",
    "For the remainder of the class, we will largely pose continuous models by explicitly defining a density function.\n",
    "\n",
    ":::{tip} Explicit Density Parameterization\n",
    "\n",
    "Define a family of random variables (e.g. a family of distributions by):\n",
    "\n",
    "1. Specifying the support of the random variable, $X$\n",
    "\n",
    "1. Specifying a density function for $X$, $f_X(x)$ that is:\n",
    "\n",
    "    - nonnegative, and\n",
    "    - normalized, and\n",
    "    - where $f_X(x)$ has some free parameters that can be adjusted to adjust its shape.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "### Example Density Functions\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "#### Uniform\n",
    "\n",
    ":::{note} Uniform (Continuous) Random Variable\n",
    "\n",
    "A random variable $X$ is continuous and **uniformly distributed** over an interval $[a,b]$ if:\n",
    "\n",
    "1. **Support:** $X \\in [a,b]$\n",
    "1. **Density:** Its PDF is constant:\n",
    "\n",
    "$$f_X(x) = \\begin{cases} \\frac{1}{|b - a|} & \\text{ if } x \\in [a,b] \\\\ 0 & \\text{ otherwise} \\end{cases} $$\n",
    "\n",
    ":::\n",
    "\n",
    "We denote the statement: *$X$ is drawn uniformly on $[a,b]$*:\n",
    "\n",
    "$$X \\sim \\text{Uni}(a,b) $$\n",
    "\n",
    "This distribution has two parameters, $a$, the lower bound of the interval of possible $X$, and $b$, the upper bound.\n",
    "\n",
    "Select \"uniform\" from the dropdown below, and experiment with the density function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a box, whose height is inversely proportional to its width. \n",
    "\n",
    "Notice that, the smaller you make the interval, the taller the density. This is a natural consequence of normalization, the area of a box is its height times its width. So, to keep the area equal to one, if must get taller as it gets narrower.\n",
    "\n",
    "Try picking $a$ and $b$ so the interval is narrower than 1. What do you notice about the height of the density function?\n",
    "\n",
    "You should see that the value of the density function is, for all $x$ inside the support, greater than 1. This can seem odd at first. Chances are always between 0 and 1. Remember, however, that $f_X(x)$ is a *density* not a chance. It is perfectly possible for densities to exceed one, so long as the total mass (area under the density curve) remains equal to one. We'll generally see that, if $X$ is a continuous random variable, and is highly likely to fall in a small interval, then the density there will be quite large. \n",
    "\n",
    "Let's try to compute the chance of an event using the uniform density.\n",
    "\n",
    ":::{tip} Example\n",
    "\n",
    "What's the chance, if $X \\sim \\text{Uni}(0,2)$, that $X \\in [0, 0.5]$?\n",
    "\n",
    "$$\\text{Pr}(X \\in [0,0.5]) = \\int_{0}^{0.5} f_X(x) dx = \\int_{0}^{0.5} \\frac{1}{2} dx = \\frac{1}{2} x|_{0}^{0.5} = \\frac{1}{4}.$$\n",
    "\n",
    "This is just probability by proportiona again. Uniform distributions model equally likely outcomes, so uniform densities, when integrated, compute the ratio of the length of the interval integrated, to the distance between the bounds $a$ and $b$.\n",
    "\n",
    ":::\n",
    "\n",
    "#### Exponential\n",
    "\n",
    ":::{note} Exponential Random Variable\n",
    "\n",
    "A random variable $X$ is **exponentially distributed** with parameter $\\lambda > 0$ if:\n",
    "\n",
    "1. **Support:** $X \\geq 0$\n",
    "1. **Density:** Its PDF is proportional to:\n",
    "\n",
    "$$f_X(x) \\propto e^{-\\lambda x} $$\n",
    "\n",
    ":::\n",
    "\n",
    "We denote the statement *$X$ is drawn from an exponential distribution with parameter $\\lambda$*:\n",
    "\n",
    "$$X \\sim \\text{Exp}(\\lambda) $$\n",
    "\n",
    "The symbol $\\propto$ in the exponential definition means \"proportional to\". When we say a density function is proportional to some other function, $g(x)$, what we mean is:\n",
    "\n",
    "$$f_X(x) = c g(x)$\n",
    "\n",
    "for some positive constant $c$. In our case,\n",
    "\n",
    "$$f_X(x) = c(\\lambda) e^{-\\lambda x}$$\n",
    "\n",
    "Notice that the constant does not depend on $x$, but does depend on the choice of the free parameter. The constant factor $c(\\lambda)$ is called the **normalizing factor**. The exponential part is the **functional form.** The functional form controls the shape of the distribution as a function of the possible values of the random variable, $x$. The normalizing constant does not depend on the random variable, but does depend on the choice of parameter, and scales the functional form. The normalizing constant is implied by the functional form and the parameter since it is introduced to ensure that $f_X(x)$ integrates to 1.\n",
    "\n",
    "In our example, if we enforce normalization, then we can solve for $c(\\lambda)$:\n",
    "\n",
    "$$\\int_{x = 0}^{\\infty} f_X(x) dx & = \\int_{0}^{\\infty} c(\\lambda) e^{-\\lambda x} dx \\\\\n",
    "& = c(\\lambda) \\int_{0}^{\\infty} e^{-\\lambda x} dx = c(\\lambda) \\left[ \\frac{-1}{\\lambda} e^{-\\lambda x} \\right}_{0}^{\\infty} \\\\\n",
    "& = \\frac{c(\\lambda)}{\\lambda} (e^{-0} - e^{-\\infty}) = \\frac{c(\\lambda)}{\\lambda}$$\n",
    "\n",
    "So, to enforce normalization, set the integral equal to one. This sets $c(\\lambda) = \\lambda$. \n",
    "\n",
    "Therefore, the exponential density always has the form:\n",
    "\n",
    "$$f_X(x) = \\begin{cases}  \\lambda e^{-\\lambda x} & \\text{ if } x \\geq 0 \\\\\n",
    "0 & \\text{ otherwise}\\end{cases} $$\n",
    "\n",
    "We could have defined the distribution this way from the start, but separating the normalizing constant from the functional form is an important skill in probability, and is helpful for directing your attention when you look at a density function. Usually, there is a normalizing constant out front, which is often a messy function of the parameters. Then, there is usually a relatively simple function of $x$ and the parameters that determines the shape of the distribution. It is the shape that controls the properties of the distribution, implies the normalizing constant, and directs when to use a given model. So, the shape is much more important. *Always read the functional form first.* \n",
    "\n",
    "In general, if $f_X(x) = c g(x)$ for some constant $c$ that depends on the parameters, then we can solve for $c$ by enforcing normalization. This gives:\n",
    "\n",
    ":::{tip} Finding Normalizing Constants\n",
    "\n",
    "If $f_X(x) \\propto g(x)$ is the density function for a continuous random variable, then $f_X(x) = c g(x)$ for some nonnegative constant $c$ where:\n",
    "\n",
    "$$ c = \\frac{1}{\\int_{-\\infty}^{\\infty} g(x) dx} $$\n",
    "\n",
    ":::\n",
    "\n",
    "Select \"exponential\" from the dropdown below and experiment with the exponential density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the density function vary as you vary $\\lambda$? \n",
    "\n",
    "You should notice that the exponential density looks a bit like the geometric PMF from [Section 2.2](ch2.2). It is nonnegative, unbounded above, decays as the input increases, and is maximized at zero. This is actually more than a chance alignment. The geometric and the exponential are analogs. We often use a geometric distribution to model continuously distributed waiting times, and the geometric to model discretely distributed waiting times. \n",
    "\n",
    "We'll talk more about the properties of the exponential and geometric, and when these are or are not reasonable models. For now, it is enough to appreciate their shape. They are a good choice for a random variable that is nonnegative, unbounded, most likely to be small, and, whose histograms only show a peak at zero then a smooth decay as we move to the right. We'll develop more precise language to describe these shapes in the coming chapters.\n",
    "\n",
    "\n",
    ":::{tip} Practice Computing a CDF\n",
    ":class: dropdown\n",
    "\n",
    "Let's practice computing a CDF. What is the CDF of the exponential distribution with $\\lambda = 1$?\n",
    "\n",
    "$$\\text{Pr}(X \\leq x) = \\int_{0}^x f_X(s) ds = \\int_{0}^x e^{-s} ds = - e^{-s} |_{0}^x  = e^{-0} - e^{-x} = 1 - e^{-x}.$$\n",
    "\n",
    "Notice that, the lower bound of integration does not come directly from the event statement $X \\leq x$. It comes from the lower bound on the support of the random variable. Make sure that, when you integrate a density, you only integrate over possible values of the random variable. Don't integrate over regions with zero density.\n",
    "\n",
    "Try drawing this CDF. Compare it to the PDF. Is your answer sensible?\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "#### Pareto\n",
    "\n",
    "Here's another basic family of continuous densities that look, to the eye, a lot like the exponential densities, but have starkly different properties. As usual, we'll explore those differences in the future. Here, we'll define the densities, study their shape, how the shape varies with parameter, and compute the normalizing constant. \n",
    "\n",
    ":::{note} Pareto Random Variable\n",
    "\n",
    "A random variable $X$ is continuous and **Pareto distributed** with parameters $x_m > 0, \\alpha > 10$ if:\n",
    "\n",
    "1. **Support:** $X \\geq x_m$\n",
    "1. **Density:** Its PDF is proportional to:\n",
    "\n",
    "$$f_X(x) \\propto \\begin{cases} x^{-(\\alpha + 1)} & \\text{ if } x \\geq x_m \\\\ 0 & \\text{ otherwise} \\end{cases} $$\n",
    "\n",
    ":::\n",
    "\n",
    "We denote the statement, *$X$ is Pareto distributed with parameters $x_m, \\alpha$*:\n",
    "\n",
    "$$X \\sim \\text{Pareto}(x_m,\\alpha) $$\n",
    "\n",
    "As for the exponential, it is easier to recognize a Pareto density by its functional form. Pareto densities are simply negative powers of $x$, cut off to force $X \\geq x_m$. They are an example of *heavy tailed* distributions. The Pareto distribution was originally introduced to model distributions of wealth. Its discrete analogs are widely used to model word frequencies, or link counts in social networks. We have introduced them here because they are the next simplest densities by functional form, they look similar to the exponential, but, behave quite differently. \n",
    "\n",
    "Select \"Pareto\" from the dropdown below and experiment with the density functions. Try changing the $\\alpha$ parameter. How does the density respond? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦ºðŸ”¨ðŸ§± Under construction. Here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see densities that look a lot like the exponential. As you vary $\\alpha$ the shape of the distribution changes. The parmeter $\\alpha$ controls the rate at which it decays for increasing inputs, and is called the *shape* parameter.\n",
    "\n",
    ":::{tip} Practice with Normalizing Constant\n",
    ":class: dropdown\n",
    "Let's work out the normalizing constant:\n",
    "\n",
    "$$\\int_{x_m}^{\\infty} x^{-(\\alpha + 1)} dx = \\frac{-1}{\\alpha} x^{-\\alpha}|_{x_m}^{\\infty} = \\frac{1}{\\alpha}(x_m^{-\\alpha} - \\infty^{-\\alpha}) = \\frac{1}{\\alpha} x_m^{-\\alpha} $$\n",
    "\n",
    "Therefore, the normalizing constant is $x_m^{\\alpha} \\alpha$ and the full density function is:\n",
    "\n",
    "$$f_X(x) \\propto \\begin{cases} x_m^{\\alpha} \\alpha x^{-(\\alpha + 1)} & \\text{ if } x \\geq x_m \\\\ 0 & \\text{ otherwise} \\end{cases} $$\n",
    ":::\n",
    "\n",
    ":::{tip} Practice Computing Chances\n",
    ":class: dropdown\n",
    "\n",
    "Now that we have the full density function, we can compute chances by integrating. \n",
    "\n",
    "Suppose $X \\sim \\text{Pareto}(1,2)$. Then:\n",
    "\n",
    "$$f_X(x) = 2 x^{-3}$$\n",
    "\n",
    "for all $x \\geq 1$.\n",
    "\n",
    "Then, to find the chance $X \\in [2,4]$, we integrate:\n",
    "\n",
    "$$\\begin{aligned} \\text{Pr}(X \\in [2,4]) & = \\int_{x = 2}^{4} 2 x^{-3} dx = -x^{-2}|_{2}^{4} \\\\& = \\frac{1}{2^2} - \\frac{1}{4^2} = \\frac{1}{4} - \\frac{1}{16} \\\\& = \\frac{3}{16}.  \\end{aligned}$$\n",
    "\n",
    ":::\n",
    "\n",
    "You will practice working with Exponential and Pareto densities on your Homework this week.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
